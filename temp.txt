https://teams.microsoft.com/l/meetup-join/19%3ameeting_NDE1ZWVmZjMtZGQxMC00OTc4LTgxZTYtOTE3NTVjZmM3YTNj%40thread.v2/0?context=%7b%22Tid%22%3a%222dfb2f0b-4d21-4268-9559-72926144c918%22%2c%22Oid%22%3a%22dc04c5a6-2a70-4053-9dff-d4fb039e698f%22%7d






import numpy as np
import pandas as pd

# -------------------------------------------------------------------
# Assumptions / join key:
# - customer 360 unique on: match_id
# - FNO txns join key: fod_clm_mtch_accnt  (customer account identifier)
# -------------------------------------------------------------------
CUST_KEY = "match_id"
FNO_KEY  = "fod_clm_mtch_accnt"

fno = txns_fno.copy()

# ----------------------------
# 1) Parse dates / canonical dt
# ----------------------------
for c in ["fod_trd_dt", "fod_trd_date", "fod_expry_dt", "fod_ordr_valid_dt"]:
    if c in fno.columns:
        fno[c] = pd.to_datetime(fno[c], errors="coerce")

# canonical trade date: prefer fod_trd_dt else fod_trd_date
fno["trade_dt"] = fno["fod_trd_dt"]
if "fod_trd_date" in fno.columns:
    fno.loc[fno["trade_dt"].isna(), "trade_dt"] = fno.loc[fno["trade_dt"].isna(), "fod_trd_date"]

# ----------------------------
# 2) Normalize flags / codes
# ----------------------------
def yn_to_bool(x):
    if pd.isna(x):
        return np.nan
    s = str(x).strip().upper()
    if s in {"Y", "YES", "1", "TRUE", "T"}: return 1
    if s in {"N", "NO", "0", "FALSE", "F"}: return 0
    return np.nan

flag_cols = [
    "fod_lmt_mrkt_sl_flg", "fod_spl_flag", "fod_fc_flag", "fod_eos_flg",
    "fod_prcimpv_flg", "fod_setlmnt_flg", "fod_1click_flg", "fod_pro_cli_ind",
    "fod_is_flg", "fod_spn_flg", "fod_source_flg"
]
for c in flag_cols:
    if c in fno.columns:
        fno[c] = fno[c].map(yn_to_bool)

# Trade flow normalization (buy/sell)
if "fod_ordr_flw" in fno.columns:
    fno["fod_ordr_flw_norm"] = fno["fod_ordr_flw"].astype(str).str.strip().str.upper()
else:
    fno["fod_ordr_flw_norm"] = np.nan

# Product / instrument normalization
for c in ["fod_prdct_typ", "fod_opt_typ", "fod_exer_typ", "fod_xchng_cd", "fod_channel", "fod_mrkt_typ"]:
    if c in fno.columns:
        fno[c] = fno[c].astype(str).str.strip().str.upper().replace({"NAN": np.nan})

# Numeric hygiene
num_cols = [
    "fod_exec_qty","fod_ordr_tot_qty","fod_cncl_qty","fod_exprd_qty",
    "fod_trd_val","fod_trd_brkg","fod_amt_blckd","fod_lss_amt_blckd",
    "fod_diff_amt_blckd","fod_diff_lss_amt_blckd","fod_trail_amt",
    "fod_span_mrgn","fod_expr_mrgn","fod_oplus_premium","fod_sltp_mrgn",
    "fod_sltp_pl","fod_sltp_sebi_mrgn","fod_strk_prc","fod_lmt_rt",
    "fod_lmt_offset","fod_sroll_diff_amt","fod_sroll_lss_amt"
]
for c in num_cols:
    if c in fno.columns:
        fno[c] = pd.to_numeric(fno[c], errors="coerce")

# ----------------------------
# 3) Helper dates
# ----------------------------
today = pd.Timestamp.today().normalize()
d30  = today - pd.Timedelta(days=30)
d90  = today - pd.Timedelta(days=90)
d180 = today - pd.Timedelta(days=180)
d365 = today - pd.Timedelta(days=365)

g = fno.groupby(FNO_KEY, dropna=False)
feat = pd.DataFrame(index=g.size().index)

# =========================================================
# A) Activity & Engagement
# =========================================================
feat["fno_ever_traded_flg"] = (g.size() > 0).astype(int)

last_trade_dt = g["trade_dt"].max()
first_trade_dt = g["trade_dt"].min()

feat["fno_last_trade_dt"] = last_trade_dt
feat["fno_first_trade_dt"] = first_trade_dt
feat["fno_days_since_last_trade"] = (today - feat["fno_last_trade_dt"]).dt.days

feat["fno_traded_last_30d_flg"] = last_trade_dt.ge(d30).astype(int)
feat["fno_traded_last_90d_flg"] = last_trade_dt.ge(d90).astype(int)
feat["fno_inactive_180d_flg"]   = (last_trade_dt.lt(d180) | last_trade_dt.isna()).astype(int)

feat["fno_trades_last_30d"] = g["trade_dt"].apply(lambda s: s.ge(d30).sum()).astype("Int64")
feat["fno_trades_last_90d"] = g["trade_dt"].apply(lambda s: s.ge(d90).sum()).astype("Int64")
feat["fno_trades_last_1y"]  = g["trade_dt"].apply(lambda s: s.ge(d365).sum()).astype("Int64")
feat["fno_total_trades"]    = g.size().astype("Int64")

tenure_months = ((today - first_trade_dt).dt.days / 30.44).replace(0, np.nan)
feat["fno_avg_trades_per_month"] = (g.size() / tenure_months).replace([np.inf, -np.inf], np.nan)

# =========================================================
# B) Value / Revenue / Cost-to-Serve (proxy from available cols)
# =========================================================
feat["fno_trade_value_total"] = g["fod_trd_val"].sum(min_count=1) if "fod_trd_val" in fno.columns else np.nan
feat["fno_trade_value_last_90d"] = g.apply(
    lambda d: d.loc[d["trade_dt"].ge(d90), "fod_trd_val"].sum(min_count=1)
) if "fod_trd_val" in fno.columns else np.nan

feat["fno_avg_trade_value"] = g["fod_trd_val"].mean() if "fod_trd_val" in fno.columns else np.nan
feat["fno_max_trade_value"] = g["fod_trd_val"].max()  if "fod_trd_val" in fno.columns else np.nan

feat["fno_total_brokerage"] = g["fod_trd_brkg"].sum(min_count=1) if "fod_trd_brkg" in fno.columns else np.nan
feat["fno_brokerage_last_90d"] = g.apply(
    lambda d: d.loc[d["trade_dt"].ge(d90), "fod_trd_brkg"].sum(min_count=1)
) if "fod_trd_brkg" in fno.columns else np.nan
feat["fno_avg_brokerage_per_trade"] = g["fod_trd_brkg"].mean() if "fod_trd_brkg" in fno.columns else np.nan

# Margin / blocked amount intensity (risk + leverage proxy)
blocked_cols = [c for c in ["fod_amt_blckd","fod_lss_amt_blckd","fod_diff_amt_blckd","fod_diff_lss_amt_blckd"] if c in fno.columns]
if blocked_cols:
    fno["blocked_total"] = fno[blocked_cols].sum(axis=1, min_count=1)
    feat["fno_margin_blocked_total"] = g["blocked_total"].sum(min_count=1)
    feat["fno_margin_blocked_last_90d"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d90), "blocked_total"].sum(min_count=1))
    feat["fno_uses_margin_flg"] = (feat["fno_margin_blocked_last_90d"].fillna(0) > 0).astype(int)
else:
    feat["fno_margin_blocked_total"] = np.nan
    feat["fno_margin_blocked_last_90d"] = np.nan
    feat["fno_uses_margin_flg"] = 0

# High-value behavior (tune threshold)
HIGH_VALUE_THRESHOLD = 100000  # adjust
feat["fno_high_value_trade_flg"] = (feat["fno_max_trade_value"].fillna(0) >= HIGH_VALUE_THRESHOLD).astype(int)

# =========================================================
# C) Buy/Sell & Order Quality
# =========================================================
feat["fno_buy_trades_cnt"] = g["fod_ordr_flw_norm"].apply(lambda s: (s == "B").sum()).astype("Int64")
feat["fno_sell_trades_cnt"] = g["fod_ordr_flw_norm"].apply(lambda s: (s == "S").sum()).astype("Int64")
feat["fno_buy_sell_ratio"] = (feat["fno_buy_trades_cnt"] / (feat["fno_sell_trades_cnt"].fillna(0) + 1)).astype(float)

feat["fno_only_buy_flg"] = ((feat["fno_buy_trades_cnt"].fillna(0) > 0) & (feat["fno_sell_trades_cnt"].fillna(0) == 0)).astype(int)
feat["fno_only_sell_flg"] = ((feat["fno_sell_trades_cnt"].fillna(0) > 0) & (feat["fno_buy_trades_cnt"].fillna(0) == 0)).astype(int)

# Cancellation / expiry ratios (execution quality / churny intent)
if "fod_ordr_tot_qty" in fno.columns:
    tot_qty = g["fod_ordr_tot_qty"].sum(min_count=1)
    cncl = g["fod_cncl_qty"].sum(min_count=1) if "fod_cncl_qty" in fno.columns else 0
    exprd = g["fod_exprd_qty"].sum(min_count=1) if "fod_exprd_qty" in fno.columns else 0
    execq = g["fod_exec_qty"].sum(min_count=1) if "fod_exec_qty" in fno.columns else np.nan

    feat["fno_total_order_qty"] = tot_qty
    feat["fno_total_exec_qty"] = execq
    feat["fno_cncl_qty_ratio"] = (cncl / tot_qty.replace(0, np.nan)).astype(float)
    feat["fno_exprd_qty_ratio"] = (exprd / tot_qty.replace(0, np.nan)).astype(float)
    feat["fno_exec_fill_ratio"] = (execq / tot_qty.replace(0, np.nan)).astype(float)
else:
    feat["fno_total_order_qty"] = np.nan
    feat["fno_total_exec_qty"] = np.nan
    feat["fno_cncl_qty_ratio"] = np.nan
    feat["fno_exprd_qty_ratio"] = np.nan
    feat["fno_exec_fill_ratio"] = np.nan

# =========================================================
# D) Product / Strategy Affinity
# =========================================================
feat["fno_distinct_underlyings"] = g["fod_undrlyng"].nunique(dropna=True).astype("Int64") if "fod_undrlyng" in fno.columns else np.nan
feat["fno_distinct_contracts"] = g["fod_cntrctnmbr"].nunique(dropna=True).astype("Int64") if "fod_cntrctnmbr" in fno.columns else np.nan

# product type affinity (top mode)
feat["fno_pref_product_type"] = g["fod_prdct_typ"].agg(
    lambda s: s.dropna().mode().iat[0] if "fod_prdct_typ" in fno.columns and not s.dropna().mode().empty else np.nan
) if "fod_prdct_typ" in fno.columns else np.nan

# option vs futures usage (based on opt_typ presence)
if "fod_opt_typ" in fno.columns:
    feat["fno_option_user_flg"] = g["fod_opt_typ"].apply(lambda s: s.notna().any()).astype(int)
    feat["fno_option_trades_cnt"] = g["fod_opt_typ"].apply(lambda s: s.notna().sum()).astype("Int64")
else:
    feat["fno_option_user_flg"] = 0
    feat["fno_option_trades_cnt"] = pd.Series(0, index=feat.index, dtype="Int64")

# strike / moneyness proxies (only meaningful for options)
if "fod_strk_prc" in fno.columns and "fod_opt_typ" in fno.columns:
    feat["fno_avg_strike_price"] = g.apply(
        lambda d: d.loc[d["fod_opt_typ"].notna(), "fod_strk_prc"].mean()
    )
else:
    feat["fno_avg_strike_price"] = np.nan

# expiry horizon (days to expiry at trade time)
if "fod_expry_dt" in fno.columns:
    fno["days_to_expiry"] = (fno["fod_expry_dt"] - fno["trade_dt"]).dt.days
    feat["fno_avg_days_to_expiry"] = g["days_to_expiry"].mean()
    feat["fno_short_dte_user_flg"] = g["days_to_expiry"].apply(lambda s: (s.le(7)).any()).astype(int)  # <= 7 days
else:
    feat["fno_avg_days_to_expiry"] = np.nan
    feat["fno_short_dte_user_flg"] = 0

# =========================================================
# E) Channel & Servicing Load
# =========================================================
if "fod_channel" in fno.columns:
    feat["fno_digital_channel_flg"] = g["fod_channel"].apply(lambda s: s.astype(str).str.contains("ONLINE|WEB|APP|MOBILE", na=False).any()).astype(int)
    feat["fno_branch_channel_flg"]  = g["fod_channel"].apply(lambda s: s.astype(str).str.contains("BRANCH", na=False).any()).astype(int)
else:
    feat["fno_digital_channel_flg"] = 0
    feat["fno_branch_channel_flg"] = 0

# 1-click / algo usage
feat["fno_1click_user_flg"] = g["fod_1click_flg"].apply(lambda s: (s == 1).any()).astype(int) if "fod_1click_flg" in fno.columns else 0
feat["fno_algo_user_flg"]   = g["fod_algo_id"].apply(lambda s: s.notna().any()).astype(int) if "fod_algo_id" in fno.columns else 0

# order modification / ack behavior (proxy for servicing complexity)
feat["fno_modification_events"] = g["fod_mdfctn_cntr"].max() if "fod_mdfctn_cntr" in fno.columns else np.nan
feat["fno_service_heavy_flg"] = (
    (feat["fno_modification_events"].fillna(0) >= 3)
    | (feat["fno_cncl_qty_ratio"].fillna(0) >= 0.3)
).astype(int)

# =========================================================
# F) Compliance / Red-Flag Proxies (limited to available cols)
# =========================================================
# missing contract number
feat["fno_contract_missing_flg"] = g["fod_cntrctnmbr"].apply(lambda s: s.isna().any()).astype(int) if "fod_cntrctnmbr" in fno.columns else np.nan

# price improvement / special flags
feat["fno_price_improve_flg"] = g["fod_prcimpv_flg"].apply(lambda s: (s == 1).any()).astype(int) if "fod_prcimpv_flg" in fno.columns else 0
feat["fno_special_flag_flg"]  = g["fod_spl_flag"].apply(lambda s: (s == 1).any()).astype(int) if "fod_spl_flag" in fno.columns else 0
feat["fno_eos_flag_flg"]      = g["fod_eos_flg"].apply(lambda s: (s == 1).any()).astype(int) if "fod_eos_flg" in fno.columns else 0

# =========================================================
# 4) Merge into customer 360
# =========================================================
customer_360_enriched = customer_360.merge(
    feat.reset_index().rename(columns={FNO_KEY: CUST_KEY}),
    how="left",
    on=CUST_KEY
)

# Fill defaults for customers with no FNO txns
flag_out = [c for c in customer_360_enriched.columns if c.endswith("_flg")]
customer_360_enriched[flag_out] = customer_360_enriched[flag_out].fillna(0).astype(int)

count_out = [c for c in customer_360_enriched.columns if c.endswith("_cnt") or c.endswith("_trades") or c.endswith("_total_trades")]
for c in count_out:
    if c in customer_360_enriched.columns:
        customer_360_enriched[c] = customer_360_enriched[c].fillna(0).astype(int)






====================================================================================






X_for_kmeans = Z  # <-- change this ONLY if your kmeans was fit on something else

labels = kmeans.labels_
centers = kmeans.cluster_centers_

# sanity check (prevents the PCA/centers mismatch error)
if X_for_kmeans.shape[1] != centers.shape[1]:
    raise ValueError(
        f"Mismatch: X_for_kmeans has {X_for_kmeans.shape[1]} features but "
        f"kmeans centers have {centers.shape[1]} features. "
        f"Set X_for_kmeans to the exact matrix used to fit kmeans."
    )

# Use PCA if dense; TruncatedSVD if sparse (works like PCA for visualization)
is_sparse = hasattr(X_for_kmeans, "tocsr") or "scipy.sparse" in str(type(X_for_kmeans)).lower()
Reducer = TruncatedSVD if is_sparse else PCA

reducer2 = Reducer(n_components=2, random_state=42)
reducer3 = Reducer(n_components=3, random_state=42)

X2 = reducer2.fit_transform(X_for_kmeans)
X3 = reducer3.fit_transform(X_for_kmeans)

# IMPORTANT: transform centers with the SAME fitted reducer2
C2 = reducer2.transform(centers)

# sample for the 5th plot (keeps it fast)
n = X2.shape[0]
sample_n = min(20000, n)
idx = np.random.default_rng(42).choice(n, size=sample_n, replace=False)

# -----------------------------
# 5 subplots in one row
# -----------------------------
fig, axes = plt.subplots(1, 5, figsize=(26, 5))

def sc(ax, x, y, title):
    ax.scatter(x, y, c=labels, s=10, alpha=0.7)
    ax.set_title(title)
    ax.set_xlabel("Comp 1")
    ax.set_ylabel("Comp 2")

# 1) 2D (comp1 vs comp2)
sc(axes[0], X2[:, 0], X2[:, 1], "2D projection (Comp1 vs Comp2)")

# 2) 3D projection: comp1 vs comp3
sc(axes[1], X3[:, 0], X3[:, 2], "3D proj (Comp1 vs Comp3)")

# 3) 3D projection: comp2 vs comp3
sc(axes[2], X3[:, 1], X3[:, 2], "3D proj (Comp2 vs Comp3)")

# 4) 2D + centroids
axes[3].scatter(X2[:, 0], X2[:, 1], c=labels, s=10, alpha=0.5)
axes[3].scatter(C2[:, 0], C2[:, 1], s=220, marker="X")
axes[3].set_title("2D + KMeans centroids")
axes[3].set_xlabel("Comp 1")
axes[3].set_ylabel("Comp 2")

# 5) 2D sampled
axes[4].scatter(X2[idx, 0], X2[idx, 1], c=np.asarray(labels)[idx], s=10, alpha=0.7)
axes[4].set_title(f"2D (sampled {sample_n:,})")
axes[4].set_xlabel("Comp 1")
axes[4].set_ylabel("Comp 2")

plt.tight_layout()
plt.show()





================================




import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Inputs expected:
# - Z: numpy array or scipy sparse matrix of shape (n_samples, n_features) used for clustering
# - kmeans: fitted KMeans (or you can fit it below)
# - OPTIONAL: df_clean for hover/labels not needed

# If you DON'T already have a fitted kmeans, uncomment and fit:
# kmeans = KMeans(n_clusters=10, random_state=42, n_init="auto")
# kmeans.fit(Z)

labels = kmeans.labels_

# PCA to 2D and 3D (3D used to create multiple 2D projections)
pca2 = PCA(n_components=2, random_state=42)
Z_pca2 = pca2.fit_transform(Z.toarray() if hasattr(Z, "toarray") else Z)

pca3 = PCA(n_components=3, random_state=42)
Z_pca3 = pca3.fit_transform(Z.toarray() if hasattr(Z, "toarray") else Z)

# Helper scatter function
def scatter2d(x, y, title):
    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, c=labels, s=10, alpha=0.7)
    plt.title(title)
    plt.xlabel("Component 1")
    plt.ylabel("Component 2")
    plt.tight_layout()
    plt.show()

# 5 x 2D visualizations
# 1) PCA 2D (PC1 vs PC2)
scatter2d(Z_pca2[:, 0], Z_pca2[:, 1], "KMeans clusters on Z (PCA 2D: PC1 vs PC2)")

# 2) PCA 3D projection: PC1 vs PC3
scatter2d(Z_pca3[:, 0], Z_pca3[:, 2], "KMeans clusters on Z (PCA: PC1 vs PC3)")

# 3) PCA 3D projection: PC2 vs PC3
scatter2d(Z_pca3[:, 1], Z_pca3[:, 2], "KMeans clusters on Z (PCA: PC2 vs PC3)")

# 4) PCA 2D with cluster centers projected into PCA space
centers = kmeans.cluster_centers_
centers_pca2 = pca2.transform(centers)

plt.figure(figsize=(8, 6))
plt.scatter(Z_pca2[:, 0], Z_pca2[:, 1], c=labels, s=10, alpha=0.6)
plt.scatter(centers_pca2[:, 0], centers_pca2[:, 1], s=200, marker="X")
plt.title("PCA 2D with KMeans centroids (projected)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.tight_layout()
plt.show()

# 5) PCA 2D, but sample points for speed (useful if Z is huge)
n = Z_pca2.shape[0]
sample_n = min(20000, n)
idx = np.random.default_rng(42).choice(n, size=sample_n, replace=False)

plt.figure(figsize=(8, 6))
plt.scatter(Z_pca2[idx, 0], Z_pca2[idx, 1], c=np.array(labels)[idx], s=10, alpha=0.7)
plt.title(f"PCA 2D (sampled {sample_n:,} points) - clusters on Z")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.tight_layout()
plt.show()




import numpy as np
import pandas as pd

# -------------------------------------------------------------------
# Assumptions / join key:
# - customers unique on: match_id
# - MF txns join key: mf_trn_match_acc  (customer account identifier)
# -------------------------------------------------------------------
CUST_KEY = "match_id"
MF_KEY   = "mf_trn_match_acc"

mf = mf_txns.copy()

# --- date parsing (use tran_cr_date as primary transaction date) ---
for c in [
    "mf_trn_tran_cr_date","mf_trn_cns_date","mf_trn_updt_date","mf_trn_rej_date",
    "mf_trn_rev_date","mf_trn_payout_dt","mf_trn_nav_date","mf_trn_download_dt",
    "mf_trn_man_updt_dt","mf_trn_upg_date","mf_trn_upld_dt","mf_trn_fifo_date",
    "mf_sipstp_reg_dt","mf_trn_rerun_dt","mf_trn_bse_payout_updt_dt",
    "mf_trn_tifd_dt","mf_ipd_dob","mf_trn_jh1_dob","mf_trn_jh2_dob"
]:
    if c in mf.columns:
        mf[c] = pd.to_datetime(mf[c], errors="coerce")

mf["trade_dt"] = mf["mf_trn_tran_cr_date"]  # canonical MF txn date

# --- normalize Y/N flags (handles Y/0/1 etc) ---
def yn_to_bool(x):
    if pd.isna(x): 
        return np.nan
    s = str(x).strip().upper()
    if s in {"Y", "YES", "1", "TRUE", "T"}: return 1
    if s in {"N", "NO", "0", "FALSE", "F"}: return 0
    return np.nan

flag_cols = [
    "mf_trn_div_reinv_flg","mf_trn_all_redeem_flg","mf_trn_high_val_flg",
    "mf_trn_download_flg","mf_trn_mail_send_flg","mf_trn_prt_flg",
    "mf_trn_trg_flg","mf_trn_swtch_flg","mf_trn_trf_swtch_div_reinv_flg",
    "mf_trn_cst_flg","mf_trn_pwg_trn_flg","mf_trn_txn_slip_flg",
    "mf_trn_email_flag","mf_iad_pan_verify_flag2","mf_iad_pan_verify_flag3",
    "mf_iid_vrfy_flg_mfkyc_cmplnt","mf_iid_vrfy_flg_pan_inv_jh1","mf_iid_vrfy_flg_pan_inv_jh2",
    "mf_exg_valid_pan","mf_exg_valid_guardn_pan","mf_exg_valid_jh1_pan","mf_exg_valid_jh2_pan",
    "mf_icd_mfocus_indian_flg","mf_trn_off_map_flg","mf_trn_fifo_flg",
    "mf_trn_rerun_flg","mf_trn_free_insure_flg","mf_trn_freed_flg",
    "mf_trn_d2us_flg","mf_trn_unblk_flg"
]
for c in flag_cols:
    if c in mf.columns:
        mf[c] = mf[c].map(yn_to_bool)

# --- helper dates ---
today = pd.Timestamp.today().normalize()
d30  = today - pd.Timedelta(days=30)
d90  = today - pd.Timedelta(days=90)
d180 = today - pd.Timedelta(days=180)
d365 = today - pd.Timedelta(days=365)

g = mf.groupby(MF_KEY, dropna=False)

feat = pd.DataFrame(index=g.size().index)

# =========================================================
# 1) MF Activity & Engagement
# =========================================================
feat["mf_ever_transacted_flg"] = (g["mf_trn_id"].count() > 0).astype(int)

feat["mf_traded_2025_flg"] = g["trade_dt"].apply(lambda s: (s.dt.year == 2025).any()).astype(int)
feat["mf_traded_last_30d_flg"] = g["trade_dt"].apply(lambda s: s.ge(d30).any()).astype(int)
feat["mf_traded_last_90d_flg"] = g["trade_dt"].apply(lambda s: s.ge(d90).any()).astype(int)

mf_last_tran_date = g["trade_dt"].max()
feat["mf_last_tran_date"] = mf_last_tran_date
feat["mf_days_since_last_tran"] = (today - feat["mf_last_tran_date"]).dt.days

feat["mf_inactive_180d_flg"] = (mf_last_tran_date.lt(d180) | mf_last_tran_date.isna()).astype(int)
feat["mf_total_txns"] = g["mf_trn_id"].count().astype("Int64")

# =========================================================
# 2) Investment Behaviour & Intent
# =========================================================
# SIP/AIP flag: treat 'A' as SIP/AIP based on your value set (A/P/blank)
feat["mf_sip_user_flg"] = g["mf_trn_aip_flg"].apply(lambda s: (s.astype(str).str.upper() == "A").any()).astype(int)
feat["mf_sip_txn_cnt"]  = g["mf_trn_aip_flg"].apply(lambda s: (s.astype(str).str.upper() == "A").sum()).astype("Int64")

feat["mf_stp_user_flg"] = g["mf_trn_stp_no"].apply(lambda s: s.notna().any()).astype(int)

# txn code behavior
feat["mf_purchase_cnt"] = g["mf_trn_cd"].apply(lambda s: (s.astype(str).str.upper() == "P").sum()).astype("Int64")
feat["mf_redeem_cnt"]   = g["mf_trn_cd"].apply(lambda s: (s.astype(str).str.upper() == "R").sum()).astype("Int64")
feat["mf_switch_cnt"]   = g["mf_trn_swtch_flg"].apply(lambda s: (s == 1).sum()).astype("Int64")

feat["mf_all_redeem_flg"] = g["mf_trn_all_redeem_flg"].apply(lambda s: (s == 1).any()).astype(int)

# =========================================================
# 3) Portfolio Depth & Diversification
# =========================================================
feat["mf_distinct_schemes"] = g["mf_trn_sch_cd"].nunique(dropna=True).astype("Int64")
feat["mf_distinct_folios"]  = g["mf_trn_folio"].nunique(dropna=True).astype("Int64")

feat["mf_multi_scheme_flg"]  = (feat["mf_distinct_schemes"].fillna(0) >= 3).astype(int)
feat["mf_single_scheme_flg"] = (feat["mf_distinct_schemes"].fillna(0) == 1).astype(int)

# =========================================================
# 4) Value & Wealth Indicators
# =========================================================
# invested = sum(mf_trn_amt where trn_cd == 'P')
feat["mf_total_invested_amt"] = g.apply(
    lambda d: d.loc[d["mf_trn_cd"].astype(str).str.upper() == "P", "mf_trn_amt"].sum(min_count=1)
)

# redeemed = sum(mf_trn_amt where trn_cd == 'R')
feat["mf_total_redeemed_amt"] = g.apply(
    lambda d: d.loc[d["mf_trn_cd"].astype(str).str.upper() == "R", "mf_trn_amt"].sum(min_count=1)
)

feat["mf_net_inflow_amt"] = feat["mf_total_invested_amt"].fillna(0) - feat["mf_total_redeemed_amt"].fillna(0)
feat["mf_avg_ticket_size"] = g["mf_trn_amt"].mean()

feat["mf_current_aum"] = g["mf_trn_aum"].max()

# high value: by flag or AUM threshold
AUM_HIGH_VALUE_THRESHOLD = 500000  # tune (e.g., 5L); change as needed
feat["mf_high_value_flg"] = (
    g["mf_trn_high_val_flg"].apply(lambda s: (s == 1).any())
    | feat["mf_current_aum"].fillna(0).ge(AUM_HIGH_VALUE_THRESHOLD)
).astype(int)

# =========================================================
# 5) Revenue & Cost to Serve
# =========================================================
feat["mf_total_brokerage"] = g["mf_trn_brokerage"].sum(min_count=1)
feat["mf_avg_brokerage_per_txn"] = g["mf_trn_brokerage"].mean()

feat["mf_commission_earned"] = g["mf_trn_comm"].sum(min_count=1)
feat["mf_total_exit_load"]   = g["mf_trn_exld_amt"].sum(min_count=1)
feat["mf_total_entry_load"]  = g["mf_trn_enld_amt"].sum(min_count=1)

# =========================================================
# 6) Risk, Churn & Red Flag Signals
# =========================================================
feat["mf_full_exit_flg"] = feat["mf_all_redeem_flg"].astype(int)

feat["mf_recent_redeem_flg_30d"] = g.apply(
    lambda d: ((d["mf_trn_cd"].astype(str).str.upper() == "R") & d["trade_dt"].ge(d30)).any()
).astype(int)

feat["mf_recent_redeem_flg_90d"] = g.apply(
    lambda d: ((d["mf_trn_cd"].astype(str).str.upper() == "R") & d["trade_dt"].ge(d90)).any()
).astype(int)

# rejected: by status code or rejection mode filled
FAIL_STATUS_SET = {"F", "R"}  # adjust if your status taxonomy differs
feat["mf_rejected_txn_flg"] = g.apply(
    lambda d: (
        d["mf_trn_status_cd"].astype(str).str.upper().isin(FAIL_STATUS_SET).any()
        or d["mf_trn_rej_mode"].notna().any()
    )
).astype(int)

feat["mf_manual_intervention_flg"] = g["mf_trn_rej_mode"].apply(
    lambda s: s.astype(str).str.upper().str.contains("MANUAL", na=False).any()
).astype(int)

# =========================================================
# 7) Channel & Servicing Load
# =========================================================
feat["mf_digital_txn_flg"] = g["mf_trn_channel_id"].apply(
    lambda s: s.astype(str).str.upper().str.contains("ONLINE", na=False).any()
).astype(int)

feat["mf_branch_txn_flg"] = g["mf_trn_channel_id"].apply(
    lambda s: s.astype(str).str.upper().str.contains("BRANCH", na=False).any()
).astype(int)

feat["mf_rm_assisted_flg"] = g["mf_trn_channel_id"].apply(
    lambda s: s.astype(str).str.upper().str.contains(r"\bRM\b", na=False).any()
).astype(int)

# service heavy: update count threshold
UPD_CNT_THRESHOLD = 3
feat["mf_service_heavy_flg"] = g["mf_trn_updt_cnt"].apply(lambda s: (s.fillna(0) > UPD_CNT_THRESHOLD).any()).astype(int)

# =========================================================
# 8) Compliance & KYC Completeness (summary)
# =========================================================
feat["mf_kyc_complete_flg"] = g["mf_iid_vrfy_flg_mfkyc_cmplnt"].apply(lambda s: (s == 1).any()).astype(int)
feat["mf_pan_verified_flg"] = g["mf_exg_valid_pan"].apply(lambda s: (s == 1).any()).astype(int)

# joint holder present: based on jh1/jh2 fields existing (Y/N or code)
def joint_present(d):
    j1 = d["mf_trn_jh1"]
    j2 = d["mf_trn_jh2"]
    # treat explicit Y/1 as present; if codes exist, any non-null can imply presence
    return ((j1 == 1).any() or (j2 == 1).any() or j1.notna().any() or j2.notna().any())

feat["mf_joint_holder_flg"] = g.apply(joint_present).astype(int)

# =========================================================
# Merge into customer 360
# =========================================================
customer_360_enriched = customers.merge(
    feat.reset_index().rename(columns={MF_KEY: CUST_KEY}),
    how="left",
    on=CUST_KEY
)

# Fill defaults for customers with no MF txns
flag_cols_out = [c for c in customer_360_enriched.columns if c.endswith("_flg")]
customer_360_enriched[flag_cols_out] = customer_360_enriched[flag_cols_out].fillna(0).astype(int)

count_cols_out = [
    "mf_total_txns","mf_sip_txn_cnt","mf_purchase_cnt","mf_redeem_cnt","mf_switch_cnt",
    "mf_distinct_schemes","mf_distinct_folios"
]
for c in count_cols_out:
    if c in customer_360_enriched.columns:
        customer_360_enriched[c] = customer_360_enriched[c].fillna(0).astype(int)







=========================================================================











import numpy as np
import pandas as pd

# Join key mapping:
# - customers unique on: match_id
# - txns unique on: trd_trd_ref
# - join customers.match_id  <->  txns.trd_clm_mtch_accnt
CUST_KEY = "match_id"
TXN_KEY  = "trd_clm_mtch_accnt"
TXN_ID_COL = "trd_trd_ref"

txns = txns.copy()

# --- ensure dates are datetime ---
txns["trd_trd_dt"] = pd.to_datetime(txns["trd_trd_dt"], errors="coerce")          # date
txns["trd_trd_dttm"] = pd.to_datetime(txns["trd_trd_dttm"], errors="coerce")      # timestamp

# Use trade date (fallback to dttm if dt missing)
txns["trade_dt"] = txns["trd_trd_dt"]
txns.loc[txns["trade_dt"].isna(), "trade_dt"] = txns.loc[txns["trade_dt"].isna(), "trd_trd_dttm"].dt.normalize()

# --- helper dates ---
today = pd.Timestamp.today().normalize()
d30  = today - pd.Timedelta(days=30)
d90  = today - pd.Timedelta(days=90)
d180 = today - pd.Timedelta(days=180)
d365 = today - pd.Timedelta(days=365)

g = txns.groupby(TXN_KEY, dropna=False)

agg = pd.DataFrame(index=g.size().index)

# =========================================================
# 1) Core Activity & Engagement Flags
# =========================================================
agg["ever_traded_flg"] = (g[TXN_ID_COL].count() > 0).astype(int)

agg["traded_in_2025_flg"] = g["trade_dt"].apply(lambda s: (s.dt.year == 2025).any()).astype(int)

last_trade_dt = g["trade_dt"].max()
agg["last_trade_dt"] = last_trade_dt

agg["traded_last_30d_flg"] = last_trade_dt.ge(d30).astype(int)
agg["traded_last_90d_flg"] = last_trade_dt.ge(d90).astype(int)
agg["inactive_180d_flg"]   = (last_trade_dt.lt(d180) | last_trade_dt.isna()).astype(int)

# =========================================================
# 2) RFM Features
# =========================================================
agg["days_since_last_trade"] = (today - agg["last_trade_dt"]).dt.days

agg["trades_last_30d"] = g["trade_dt"].apply(lambda s: s.ge(d30).sum()).astype("Int64")
agg["trades_last_90d"] = g["trade_dt"].apply(lambda s: s.ge(d90).sum()).astype("Int64")
agg["trades_last_1y"]  = g["trade_dt"].apply(lambda s: s.ge(d365).sum()).astype("Int64")

first_trade_dt = g["trade_dt"].min()
agg["first_trade_dt"] = first_trade_dt

tenure_months = ((today - first_trade_dt).dt.days / 30.44).replace(0, np.nan)
agg["avg_trades_per_month"] = (g.size() / tenure_months).replace([np.inf, -np.inf], np.nan)

agg["trade_value_last_1y"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d365), "trd_trd_vl"].sum(min_count=1))
agg["avg_trade_value"] = g["trd_trd_vl"].mean()
agg["max_trade_value"] = g["trd_trd_vl"].max()

# =========================================================
# 3) Buy/Sell Behaviour Signals
# =========================================================
agg["buy_trades_cnt"]  = g["trd_trd_flw"].apply(lambda s: (s == "B").sum()).astype("Int64")
agg["sell_trades_cnt"] = g["trd_trd_flw"].apply(lambda s: (s == "S").sum()).astype("Int64")

agg["buy_sell_ratio"] = (agg["buy_trades_cnt"] / (agg["sell_trades_cnt"].fillna(0) + 1)).astype(float)

agg["only_buy_flg"]  = ((agg["buy_trades_cnt"].fillna(0) > 0) & (agg["sell_trades_cnt"].fillna(0) == 0)).astype(int)
agg["only_sell_flg"] = ((agg["sell_trades_cnt"].fillna(0) > 0) & (agg["buy_trades_cnt"].fillna(0) == 0)).astype(int)

# =========================================================
# 4) Product & Instrument Affinity
# =========================================================
agg["distinct_stocks_traded"] = g["trd_stck_cd"].nunique(dropna=True).astype("Int64")

agg["top_traded_stock"] = g["trd_stck_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["single_stock_flg"] = (agg["distinct_stocks_traded"] == 1).astype(int)
agg["multi_stock_flg"]  = (agg["distinct_stocks_traded"].fillna(0) >= 5).astype(int)  # threshold example: 5+

agg["pref_exchange"] = g["trd_xchng_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["pref_segment"] = g["trd_xchng_sgmnt_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

# =========================================================
# 5) Risk & Value Indicators
# =========================================================
# Choose a sensible threshold for your business. Example: INR 1,00,000.
HIGH_VALUE_THRESHOLD = 100000
agg["high_value_trade_flg"] = (agg["max_trade_value"].fillna(0) >= HIGH_VALUE_THRESHOLD).astype(int)

agg["margin_blocked_last_90d"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d90), "trd_amt_blckd"].sum(min_count=1))
agg["uses_margin_flg"] = (agg["margin_blocked_last_90d"].fillna(0) > 0).astype(int)

agg["active_trade_days"] = g["trade_dt"].agg(lambda s: s.dt.normalize().nunique())
agg["trade_value_total"] = g["trd_trd_vl"].sum(min_count=1)
agg["avg_daily_trade_value"] = agg["trade_value_total"] / agg["active_trade_days"].replace(0, np.nan)

# =========================================================
# 6) Cost & Revenue Features
# =========================================================
agg["total_brokerage_lifetime"] = g["trd_brkrg_vl"].sum(min_count=1)
agg["brokerage_last_90d"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d90), "trd_brkrg_vl"].sum(min_count=1))
agg["avg_brokerage_per_trade"] = g["trd_brkrg_vl"].mean()
agg["zero_brokerage_flg"] = (agg["avg_brokerage_per_trade"].fillna(0) == 0).astype(int)

# Optional extra revenue/charge rollups (derived from available cols; not dropping anything)
agg["total_trnx_charges_lifetime"] = g["trd_trnx_chrg"].sum(min_count=1)
agg["total_stamp_duty_lifetime"]   = g["trd_stmp_duty"].sum(min_count=1)
agg["total_sebi_charges_lifetime"] = g["trd_sebi_chrg_val"].sum(min_count=1)
agg["total_stt_lifetime"]          = g["trd_stt"].sum(min_count=1)
agg["total_tax_gst_lifetime"]      = (
    g["trd_cgst_amt"].sum(min_count=1)
    + g["trd_sgst_amt"].sum(min_count=1)
    + g["trd_ugst_amt"].sum(min_count=1)
    + g["trd_igst_amt"].sum(min_count=1)
)

# Brokerage composition
agg["total_fixed_brkg_lifetime"]    = g["trd_fixed_brkg"].sum(min_count=1)
agg["total_variable_brkg_lifetime"] = g["trd_variable_brkg"].sum(min_count=1)

agg["pref_brkrg_model"] = g["trd_brkrg_mdl"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["pref_brkrg_type"] = g["trd_brkrg_typ"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

# =========================================================
# 7) Compliance & Exception Flags (only where feasible from your columns)
# =========================================================
agg["contract_missing_flg"] = g["trd_cntrct_nmbr"].apply(lambda s: s.isna().any()).astype(int)

# Exchange mismatch: if any row has upload/match flag not equal to 'P'
agg["exchange_mismatch_flg"] = g["trd_upld_mtch_flg"].apply(lambda s: (~s.isin(["P"])).any()).astype(int)

# has_failed_trades_flg: IMPOSSIBLE from the provided columns (no clear success/fail status field)
agg["has_failed_trades_flg"] = np.nan

# =========================================================
# Merge into customer 360
# =========================================================
customer_360_enriched = customers.merge(
    agg.reset_index().rename(columns={TXN_KEY: CUST_KEY}),
    how="left",
    on=CUST_KEY
)

# Fill defaults for customers with no trades
flag_cols = [c for c in customer_360_enriched.columns if c.endswith("_flg")]
customer_360_enriched[flag_cols] = customer_360_enriched[flag_cols].fillna(0).astype(int)

count_cols = [
    "trades_last_30d","trades_last_90d","trades_last_1y",
    "buy_trades_cnt","sell_trades_cnt","distinct_stocks_traded",
    "active_trade_days"
]
for c in count_cols:
    if c in customer_360_enriched.columns:
        customer_360_enriched[c] = customer_360_enriched[c].fillna(0).astype(int)





===============================================================================






"""
Large-table-friendly EDA for a Customer 360 table
- Works with: pandas DataFrame OR SQL table via SQLAlchemy
- Strategy: sampling + pushdown aggregations (when SQL) + robust type/missing/cardinality checks
"""

from __future__ import annotations

import math
import re
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import numpy as np
import pandas as pd


# ---------------------------
# Helpers: formatting & types
# ---------------------------

def _is_probably_id(col: str) -> bool:
    """Heuristic to identify ID-like columns by name."""
    col_l = col.lower()
    patterns = [r"\bid\b", r"_id\b", r"\bkey\b", r"\buid\b", r"\bcust\b", r"\bcustomer\b", r"\baccount\b"]
    return any(re.search(p, col_l) for p in patterns)

def _series_type_label(s: pd.Series) -> str:
    if pd.api.types.is_bool_dtype(s):
        return "bool"
    if pd.api.types.is_numeric_dtype(s):
        return "numeric"
    if pd.api.types.is_datetime64_any_dtype(s):
        return "datetime"
    if pd.api.types.is_categorical_dtype(s):
        return "category"
    return "text"

def _safe_value_counts(s: pd.Series, top_n: int = 10) -> pd.DataFrame:
    vc = s.value_counts(dropna=False).head(top_n)
    out = vc.reset_index()
    out.columns = ["value", "count"]
    out["pct"] = out["count"] / out["count"].sum()
    return out

def _approx_mem_mb(df: pd.DataFrame) -> float:
    return float(df.memory_usage(deep=True).sum()) / (1024**2)


# ---------------------------
# Data source abstraction
# ---------------------------

@dataclass
class SQLSource:
    """
    Wraps a SQLAlchemy engine/connection and a table or query.
    Provide either:
      - table="schema.table"
      - query="SELECT ... FROM ..."
    """
    engine: Any  # sqlalchemy.Engine or Connection
    table: Optional[str] = None
    query: Optional[str] = None
    schema: Optional[str] = None

    def base_sql(self) -> str:
        if self.query:
            return f"({self.query}) AS t"
        if self.table:
            if self.schema and "." not in self.table:
                return f"{self.schema}.{self.table}"
            return self.table
        raise ValueError("Provide either table or query.")

    def sample_df(self, n: int = 100_000, seed: int = 42) -> pd.DataFrame:
        """
        Returns a sample as a pandas DataFrame.
        Generic approach: ORDER BY random() (Postgres) is not portable.
        Use a simple LIMIT for portability; for huge tables you may want db-specific sampling.
        """
        sql = f"SELECT * FROM {self.base_sql()} LIMIT {int(n)}"
        return pd.read_sql(sql, self.engine)

    def row_count(self) -> int:
        sql = f"SELECT COUNT(*) AS n FROM {self.base_sql()}"
        return int(pd.read_sql(sql, self.engine).iloc[0, 0])

    def missingness(self) -> pd.DataFrame:
        """
        Null counts per column (pushdown). Uses information_schema not required; uses SUM(CASE WHEN col IS NULL THEN 1 END).
        For wide tables this query can be big but still usually faster than pulling data.
        """
        # Fetch column names via a 0-row query
        cols = pd.read_sql(f"SELECT * FROM {self.base_sql()} WHERE 1=0", self.engine).columns.tolist()
        exprs = [f"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}" for c in cols]
        sql = f"SELECT {', '.join(exprs)} FROM {self.base_sql()}"
        nulls = pd.read_sql(sql, self.engine).iloc[0].to_frame("nulls")
        nulls["col"] = nulls.index
        nulls = nulls.reset_index(drop=True)[["col", "nulls"]]
        total = self.row_count()
        nulls["pct_null"] = nulls["nulls"] / total if total else np.nan
        return nulls.sort_values("pct_null", ascending=False)


# ---------------------------
# Core EDA for a pandas sample
# ---------------------------

def profile_dataframe(
    df: pd.DataFrame,
    *,
    customer_id_col: Optional[str] = None,
    top_n: int = 10,
    infer_datetime: bool = True
) -> Dict[str, Any]:
    """
    Profiles a DataFrame (ideally a sample of your big table).
    Returns dict of summary tables: columns, numeric_stats, duplicates, top_values, etc.
    """
    out: Dict[str, Any] = {}

    # Basic table summary
    out["table_summary"] = pd.DataFrame([{
        "rows": len(df),
        "cols": df.shape[1],
        "memory_mb_est": round(_approx_mem_mb(df), 2)
    }])

    # Optionally infer datetime for object columns that look like dates
    if infer_datetime:
        for c in df.select_dtypes(include=["object"]).columns:
            # quick heuristic: try parse a small sample
            sample = df[c].dropna().astype(str).head(200)
            if len(sample) == 0:
                continue
            parsed = pd.to_datetime(sample, errors="coerce", utc=False)
            if parsed.notna().mean() >= 0.8:
                df[c] = pd.to_datetime(df[c], errors="coerce")

    # Column-level profile
    prof_rows = []
    for c in df.columns:
        s = df[c]
        typ = _series_type_label(s)
        n = len(s)
        n_null = int(s.isna().sum())
        n_nonnull = n - n_null
        n_unique = int(s.nunique(dropna=True))
        unique_rate = (n_unique / n_nonnull) if n_nonnull else np.nan

        prof_rows.append({
            "col": c,
            "type": typ,
            "nulls": n_null,
            "pct_null": (n_null / n) if n else np.nan,
            "unique": n_unique,
            "unique_rate_nonnull": unique_rate,
            "is_probable_id": _is_probably_id(c) or (customer_id_col == c),
        })

    col_profile = pd.DataFrame(prof_rows).sort_values(["pct_null", "unique_rate_nonnull"], ascending=[False, False])
    out["column_profile"] = col_profile

    # Duplicates
    dup_rows = int(df.duplicated().sum())
    out["duplicate_rows"] = pd.DataFrame([{
        "duplicate_rows": dup_rows,
        "pct_duplicate_rows": (dup_rows / len(df)) if len(df) else np.nan
    }])

    # Customer ID duplication / uniqueness (if provided or inferred)
    cid = customer_id_col
    if cid is None:
        # try to infer a likely id col
        candidates = col_profile.sort_values(["is_probable_id", "unique_rate_nonnull"], ascending=[False, False])
        cid = candidates.iloc[0]["col"] if len(candidates) else None

    if cid and cid in df.columns:
        s = df[cid]
        out["customer_id_quality"] = pd.DataFrame([{
            "customer_id_col": cid,
            "nulls": int(s.isna().sum()),
            "unique_nonnull": int(s.nunique(dropna=True)),
            "rows": len(s),
            "pct_rows_unique_nonnull": (s.nunique(dropna=True) / (len(s) - s.isna().sum())) if (len(s) - s.isna().sum()) else np.nan,
            "duplicate_ids_nonnull": int(s.duplicated().sum()),
        }])
    else:
        out["customer_id_quality"] = pd.DataFrame([{
            "customer_id_col": None,
            "note": "No customer_id_col provided/found."
        }])

    # Numeric stats
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if num_cols:
        desc = df[num_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]).T
        desc["missing"] = df[num_cols].isna().sum()
        desc["pct_missing"] = desc["missing"] / len(df)
        out["numeric_stats"] = desc.reset_index(names="col").sort_values("pct_missing", ascending=False)
    else:
        out["numeric_stats"] = pd.DataFrame(columns=["col"])

    # Datetime range
    dt_cols = df.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
    if dt_cols:
        dt_rows = []
        for c in dt_cols:
            s = df[c]
            dt_rows.append({
                "col": c,
                "min": s.min(),
                "max": s.max(),
                "nulls": int(s.isna().sum()),
                "pct_null": float(s.isna().mean()),
            })
        out["datetime_ranges"] = pd.DataFrame(dt_rows).sort_values("pct_null", ascending=False)
    else:
        out["datetime_ranges"] = pd.DataFrame(columns=["col", "min", "max", "nulls", "pct_null"])

    # Categorical/text top values
    cat_cols = [c for c in df.columns if _series_type_label(df[c]) in ("text", "category", "bool")]
    top_values: Dict[str, pd.DataFrame] = {}
    for c in cat_cols[:200]:  # avoid blowing up on ultra-wide tables
        s = df[c]
        # skip ultra-high cardinality columns unless ID-like
        if (s.nunique(dropna=True) > 5000) and (not _is_probably_id(c)):
            continue
        top_values[c] = _safe_value_counts(s, top_n=top_n)

    out["top_values"] = top_values

    # Quick correlation (numeric only, limited columns)
    if len(num_cols) >= 2:
        limited = num_cols[:50]  # cap to avoid huge matrix
        corr = df[limited].corr(numeric_only=True)
        out["corr_numeric"] = corr
    else:
        out["corr_numeric"] = pd.DataFrame()

    return out


# ---------------------------
# Orchestrator: df or SQL
# ---------------------------

def run_customer360_eda(
    data: Union[pd.DataFrame, SQLSource],
    *,
    sample_rows: int = 200_000,
    seed: int = 42,
    customer_id_col: Optional[str] = None,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    Main entry point.
    - If data is SQLSource: pulls a sample + runs pushdown row count & missingness if feasible.
    - If data is DataFrame: runs profiling directly (optionally sample if huge).
    """
    results: Dict[str, Any] = {}

    if isinstance(data, SQLSource):
        # SQL pushdowns
        try:
            results["row_count"] = pd.DataFrame([{"rows_total": data.row_count()}])
        except Exception as e:
            results["row_count"] = pd.DataFrame([{"rows_total": None, "error": str(e)}])

        try:
            results["missingness_sql"] = data.missingness()
        except Exception as e:
            results["missingness_sql"] = pd.DataFrame([{"error": str(e)}])

        # Sample to pandas for richer profiling
        df_sample = data.sample_df(n=sample_rows, seed=seed)
        results["sample_table_summary"] = pd.DataFrame([{
            "sample_rows": len(df_sample),
            "sample_cols": df_sample.shape[1],
            "sample_memory_mb_est": round(_approx_mem_mb(df_sample), 2)
        }])
        results.update({f"sample_{k}": v for k, v in profile_dataframe(
            df_sample, customer_id_col=customer_id_col, top_n=top_n
        ).items()})

        return results

    # Pandas DataFrame path
    df = data
    if len(df) > sample_rows:
        df = df.sample(n=sample_rows, random_state=seed)
        results["note"] = f"Input df had >{sample_rows:,} rows; profiled a sample of {len(df):,}."

    results.update(profile_dataframe(df, customer_id_col=customer_id_col, top_n=top_n))
    return results


# ---------------------------
# Example usage
# ---------------------------

if __name__ == "__main__":
    # ---- Option A: Already have a DataFrame df ----
    # results = run_customer360_eda(df, customer_id_col="customer_id")

    # ---- Option B: SQL table / query ----
    # pip install sqlalchemy psycopg2-binary (for Postgres), or pyodbc (for SQL Server), etc.
    from sqlalchemy import create_engine

    # Example (Postgres):
    # engine = create_engine("postgresql+psycopg2://user:password@host:5432/dbname")
    # src = SQLSource(engine=engine, table="customer360", schema="public")
    # results = run_customer360_eda(src, sample_rows=200_000, customer_id_col="customer_id")

    # Print a few key outputs if you run it
    # print(results["table_summary"])
    # print(results["column_profile"].head(25))
    pass


1Ô∏è‚É£ Use your existing pyodbc connection (unchanged login)
import pyodbc

conn = pyodbc.connect(
    f"DSN=DatalakeImpala;Schema=analyticsdb;UID={username};PWD={password}",
    autocommit=True
)

cursor = conn.cursor()


We‚Äôll reuse conn directly for pandas reads.

2Ô∏è‚É£ Replace SQLAlchemy with a pyodbc-based SQLSource

Here is a Impala-safe SQLSource that works with pyodbc + pandas.

import pandas as pd
import numpy as np
from dataclasses import dataclass
from typing import Optional


@dataclass
class ImpalaSQLSource:
    """
    SQL source backed by a pyodbc Impala connection
    """
    conn: pyodbc.Connection
    table: Optional[str] = None
    query: Optional[str] = None

    def base_sql(self) -> str:
        if self.query:
            return f"({self.query}) t"
        if self.table:
            return self.table
        raise ValueError("Provide either table or query")

    def sample_df(self, n: int = 100_000) -> pd.DataFrame:
        """
        Impala-friendly sampling:
        - LIMIT only (ORDER BY RAND() is expensive in Impala)
        """
        sql = f"""
        SELECT *
        FROM {self.base_sql()}
        LIMIT {n}
        """
        return pd.read_sql(sql, self.conn)

    def row_count(self) -> int:
        sql = f"""
        SELECT COUNT(*) AS n
        FROM {self.base_sql()}
        """
        return int(pd.read_sql(sql, self.conn).iloc[0, 0])

    def missingness(self) -> pd.DataFrame:
        """
        Column-wise null counts pushed to Impala
        """
        # Get columns without scanning data
        cols_sql = f"""
        SELECT *
        FROM {self.base_sql()}
        LIMIT 0
        """
        cols = pd.read_sql(cols_sql, self.conn).columns.tolist()

        null_exprs = [
            f"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}"
            for c in cols
        ]

        sql = f"""
        SELECT {", ".join(null_exprs)}
        FROM {self.base_sql()}
        """

        nulls = pd.read_sql(sql, self.conn).iloc[0].to_frame("nulls")
        nulls["col"] = nulls.index
        nulls = nulls.reset_index(drop=True)[["col", "nulls"]]

        total_rows = self.row_count()
        nulls["pct_null"] = nulls["nulls"] / total_rows

        return nulls.sort_values("pct_null", ascending=False)

3Ô∏è‚É£ Updated example usage (Customer 360 table)
üëâ Table-based usage
src = ImpalaSQLSource(
    conn=conn,
    table="analyticsdb.customer_360"
)

results = run_customer360_eda(
    data=src,
    sample_rows=200_000,
    customer_id_col="customer_id"
)

üëâ Query-based usage (recommended for filtering early)
src = ImpalaSQLSource(
    conn=conn,
    query="""
        SELECT *
        FROM analyticsdb.customer_360
        WHERE is_active = 1
    """
)

results = run_customer360_eda(
    data=src,
    sample_rows=150_000,
    customer_id_col="customer_id"
)






