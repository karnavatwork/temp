

import numpy as np
import pandas as pd

# Join key mapping:
# - customers unique on: match_id
# - txns unique on: trd_trd_ref
# - join customers.match_id  <->  txns.trd_clm_mtch_accnt
CUST_KEY = "match_id"
TXN_KEY  = "trd_clm_mtch_accnt"
TXN_ID_COL = "trd_trd_ref"

txns = txns.copy()

# --- ensure dates are datetime ---
txns["trd_trd_dt"] = pd.to_datetime(txns["trd_trd_dt"], errors="coerce")          # date
txns["trd_trd_dttm"] = pd.to_datetime(txns["trd_trd_dttm"], errors="coerce")      # timestamp

# Use trade date (fallback to dttm if dt missing)
txns["trade_dt"] = txns["trd_trd_dt"]
txns.loc[txns["trade_dt"].isna(), "trade_dt"] = txns.loc[txns["trade_dt"].isna(), "trd_trd_dttm"].dt.normalize()

# --- helper dates ---
today = pd.Timestamp.today().normalize()
d30  = today - pd.Timedelta(days=30)
d90  = today - pd.Timedelta(days=90)
d180 = today - pd.Timedelta(days=180)
d365 = today - pd.Timedelta(days=365)

g = txns.groupby(TXN_KEY, dropna=False)

agg = pd.DataFrame(index=g.size().index)

# =========================================================
# 1) Core Activity & Engagement Flags
# =========================================================
agg["ever_traded_flg"] = (g[TXN_ID_COL].count() > 0).astype(int)

agg["traded_in_2025_flg"] = g["trade_dt"].apply(lambda s: (s.dt.year == 2025).any()).astype(int)

last_trade_dt = g["trade_dt"].max()
agg["last_trade_dt"] = last_trade_dt

agg["traded_last_30d_flg"] = last_trade_dt.ge(d30).astype(int)
agg["traded_last_90d_flg"] = last_trade_dt.ge(d90).astype(int)
agg["inactive_180d_flg"]   = (last_trade_dt.lt(d180) | last_trade_dt.isna()).astype(int)

# =========================================================
# 2) RFM Features
# =========================================================
agg["days_since_last_trade"] = (today - agg["last_trade_dt"]).dt.days

agg["trades_last_30d"] = g["trade_dt"].apply(lambda s: s.ge(d30).sum()).astype("Int64")
agg["trades_last_90d"] = g["trade_dt"].apply(lambda s: s.ge(d90).sum()).astype("Int64")
agg["trades_last_1y"]  = g["trade_dt"].apply(lambda s: s.ge(d365).sum()).astype("Int64")

first_trade_dt = g["trade_dt"].min()
agg["first_trade_dt"] = first_trade_dt

tenure_months = ((today - first_trade_dt).dt.days / 30.44).replace(0, np.nan)
agg["avg_trades_per_month"] = (g.size() / tenure_months).replace([np.inf, -np.inf], np.nan)

agg["trade_value_last_1y"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d365), "trd_trd_vl"].sum(min_count=1))
agg["avg_trade_value"] = g["trd_trd_vl"].mean()
agg["max_trade_value"] = g["trd_trd_vl"].max()

# =========================================================
# 3) Buy/Sell Behaviour Signals
# =========================================================
agg["buy_trades_cnt"]  = g["trd_trd_flw"].apply(lambda s: (s == "B").sum()).astype("Int64")
agg["sell_trades_cnt"] = g["trd_trd_flw"].apply(lambda s: (s == "S").sum()).astype("Int64")

agg["buy_sell_ratio"] = (agg["buy_trades_cnt"] / (agg["sell_trades_cnt"].fillna(0) + 1)).astype(float)

agg["only_buy_flg"]  = ((agg["buy_trades_cnt"].fillna(0) > 0) & (agg["sell_trades_cnt"].fillna(0) == 0)).astype(int)
agg["only_sell_flg"] = ((agg["sell_trades_cnt"].fillna(0) > 0) & (agg["buy_trades_cnt"].fillna(0) == 0)).astype(int)

# =========================================================
# 4) Product & Instrument Affinity
# =========================================================
agg["distinct_stocks_traded"] = g["trd_stck_cd"].nunique(dropna=True).astype("Int64")

agg["top_traded_stock"] = g["trd_stck_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["single_stock_flg"] = (agg["distinct_stocks_traded"] == 1).astype(int)
agg["multi_stock_flg"]  = (agg["distinct_stocks_traded"].fillna(0) >= 5).astype(int)  # threshold example: 5+

agg["pref_exchange"] = g["trd_xchng_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["pref_segment"] = g["trd_xchng_sgmnt_cd"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

# =========================================================
# 5) Risk & Value Indicators
# =========================================================
# Choose a sensible threshold for your business. Example: INR 1,00,000.
HIGH_VALUE_THRESHOLD = 100000
agg["high_value_trade_flg"] = (agg["max_trade_value"].fillna(0) >= HIGH_VALUE_THRESHOLD).astype(int)

agg["margin_blocked_last_90d"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d90), "trd_amt_blckd"].sum(min_count=1))
agg["uses_margin_flg"] = (agg["margin_blocked_last_90d"].fillna(0) > 0).astype(int)

agg["active_trade_days"] = g["trade_dt"].agg(lambda s: s.dt.normalize().nunique())
agg["trade_value_total"] = g["trd_trd_vl"].sum(min_count=1)
agg["avg_daily_trade_value"] = agg["trade_value_total"] / agg["active_trade_days"].replace(0, np.nan)

# =========================================================
# 6) Cost & Revenue Features
# =========================================================
agg["total_brokerage_lifetime"] = g["trd_brkrg_vl"].sum(min_count=1)
agg["brokerage_last_90d"] = g.apply(lambda d: d.loc[d["trade_dt"].ge(d90), "trd_brkrg_vl"].sum(min_count=1))
agg["avg_brokerage_per_trade"] = g["trd_brkrg_vl"].mean()
agg["zero_brokerage_flg"] = (agg["avg_brokerage_per_trade"].fillna(0) == 0).astype(int)

# Optional extra revenue/charge rollups (derived from available cols; not dropping anything)
agg["total_trnx_charges_lifetime"] = g["trd_trnx_chrg"].sum(min_count=1)
agg["total_stamp_duty_lifetime"]   = g["trd_stmp_duty"].sum(min_count=1)
agg["total_sebi_charges_lifetime"] = g["trd_sebi_chrg_val"].sum(min_count=1)
agg["total_stt_lifetime"]          = g["trd_stt"].sum(min_count=1)
agg["total_tax_gst_lifetime"]      = (
    g["trd_cgst_amt"].sum(min_count=1)
    + g["trd_sgst_amt"].sum(min_count=1)
    + g["trd_ugst_amt"].sum(min_count=1)
    + g["trd_igst_amt"].sum(min_count=1)
)

# Brokerage composition
agg["total_fixed_brkg_lifetime"]    = g["trd_fixed_brkg"].sum(min_count=1)
agg["total_variable_brkg_lifetime"] = g["trd_variable_brkg"].sum(min_count=1)

agg["pref_brkrg_model"] = g["trd_brkrg_mdl"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

agg["pref_brkrg_type"] = g["trd_brkrg_typ"].agg(
    lambda s: s.dropna().mode().iat[0] if not s.dropna().mode().empty else np.nan
)

# =========================================================
# 7) Compliance & Exception Flags (only where feasible from your columns)
# =========================================================
agg["contract_missing_flg"] = g["trd_cntrct_nmbr"].apply(lambda s: s.isna().any()).astype(int)

# Exchange mismatch: if any row has upload/match flag not equal to 'P'
agg["exchange_mismatch_flg"] = g["trd_upld_mtch_flg"].apply(lambda s: (~s.isin(["P"])).any()).astype(int)

# has_failed_trades_flg: IMPOSSIBLE from the provided columns (no clear success/fail status field)
agg["has_failed_trades_flg"] = np.nan

# =========================================================
# Merge into customer 360
# =========================================================
customer_360_enriched = customers.merge(
    agg.reset_index().rename(columns={TXN_KEY: CUST_KEY}),
    how="left",
    on=CUST_KEY
)

# Fill defaults for customers with no trades
flag_cols = [c for c in customer_360_enriched.columns if c.endswith("_flg")]
customer_360_enriched[flag_cols] = customer_360_enriched[flag_cols].fillna(0).astype(int)

count_cols = [
    "trades_last_30d","trades_last_90d","trades_last_1y",
    "buy_trades_cnt","sell_trades_cnt","distinct_stocks_traded",
    "active_trade_days"
]
for c in count_cols:
    if c in customer_360_enriched.columns:
        customer_360_enriched[c] = customer_360_enriched[c].fillna(0).astype(int)





===============================================================================






"""
Large-table-friendly EDA for a Customer 360 table
- Works with: pandas DataFrame OR SQL table via SQLAlchemy
- Strategy: sampling + pushdown aggregations (when SQL) + robust type/missing/cardinality checks
"""

from __future__ import annotations

import math
import re
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import numpy as np
import pandas as pd


# ---------------------------
# Helpers: formatting & types
# ---------------------------

def _is_probably_id(col: str) -> bool:
    """Heuristic to identify ID-like columns by name."""
    col_l = col.lower()
    patterns = [r"\bid\b", r"_id\b", r"\bkey\b", r"\buid\b", r"\bcust\b", r"\bcustomer\b", r"\baccount\b"]
    return any(re.search(p, col_l) for p in patterns)

def _series_type_label(s: pd.Series) -> str:
    if pd.api.types.is_bool_dtype(s):
        return "bool"
    if pd.api.types.is_numeric_dtype(s):
        return "numeric"
    if pd.api.types.is_datetime64_any_dtype(s):
        return "datetime"
    if pd.api.types.is_categorical_dtype(s):
        return "category"
    return "text"

def _safe_value_counts(s: pd.Series, top_n: int = 10) -> pd.DataFrame:
    vc = s.value_counts(dropna=False).head(top_n)
    out = vc.reset_index()
    out.columns = ["value", "count"]
    out["pct"] = out["count"] / out["count"].sum()
    return out

def _approx_mem_mb(df: pd.DataFrame) -> float:
    return float(df.memory_usage(deep=True).sum()) / (1024**2)


# ---------------------------
# Data source abstraction
# ---------------------------

@dataclass
class SQLSource:
    """
    Wraps a SQLAlchemy engine/connection and a table or query.
    Provide either:
      - table="schema.table"
      - query="SELECT ... FROM ..."
    """
    engine: Any  # sqlalchemy.Engine or Connection
    table: Optional[str] = None
    query: Optional[str] = None
    schema: Optional[str] = None

    def base_sql(self) -> str:
        if self.query:
            return f"({self.query}) AS t"
        if self.table:
            if self.schema and "." not in self.table:
                return f"{self.schema}.{self.table}"
            return self.table
        raise ValueError("Provide either table or query.")

    def sample_df(self, n: int = 100_000, seed: int = 42) -> pd.DataFrame:
        """
        Returns a sample as a pandas DataFrame.
        Generic approach: ORDER BY random() (Postgres) is not portable.
        Use a simple LIMIT for portability; for huge tables you may want db-specific sampling.
        """
        sql = f"SELECT * FROM {self.base_sql()} LIMIT {int(n)}"
        return pd.read_sql(sql, self.engine)

    def row_count(self) -> int:
        sql = f"SELECT COUNT(*) AS n FROM {self.base_sql()}"
        return int(pd.read_sql(sql, self.engine).iloc[0, 0])

    def missingness(self) -> pd.DataFrame:
        """
        Null counts per column (pushdown). Uses information_schema not required; uses SUM(CASE WHEN col IS NULL THEN 1 END).
        For wide tables this query can be big but still usually faster than pulling data.
        """
        # Fetch column names via a 0-row query
        cols = pd.read_sql(f"SELECT * FROM {self.base_sql()} WHERE 1=0", self.engine).columns.tolist()
        exprs = [f"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}" for c in cols]
        sql = f"SELECT {', '.join(exprs)} FROM {self.base_sql()}"
        nulls = pd.read_sql(sql, self.engine).iloc[0].to_frame("nulls")
        nulls["col"] = nulls.index
        nulls = nulls.reset_index(drop=True)[["col", "nulls"]]
        total = self.row_count()
        nulls["pct_null"] = nulls["nulls"] / total if total else np.nan
        return nulls.sort_values("pct_null", ascending=False)


# ---------------------------
# Core EDA for a pandas sample
# ---------------------------

def profile_dataframe(
    df: pd.DataFrame,
    *,
    customer_id_col: Optional[str] = None,
    top_n: int = 10,
    infer_datetime: bool = True
) -> Dict[str, Any]:
    """
    Profiles a DataFrame (ideally a sample of your big table).
    Returns dict of summary tables: columns, numeric_stats, duplicates, top_values, etc.
    """
    out: Dict[str, Any] = {}

    # Basic table summary
    out["table_summary"] = pd.DataFrame([{
        "rows": len(df),
        "cols": df.shape[1],
        "memory_mb_est": round(_approx_mem_mb(df), 2)
    }])

    # Optionally infer datetime for object columns that look like dates
    if infer_datetime:
        for c in df.select_dtypes(include=["object"]).columns:
            # quick heuristic: try parse a small sample
            sample = df[c].dropna().astype(str).head(200)
            if len(sample) == 0:
                continue
            parsed = pd.to_datetime(sample, errors="coerce", utc=False)
            if parsed.notna().mean() >= 0.8:
                df[c] = pd.to_datetime(df[c], errors="coerce")

    # Column-level profile
    prof_rows = []
    for c in df.columns:
        s = df[c]
        typ = _series_type_label(s)
        n = len(s)
        n_null = int(s.isna().sum())
        n_nonnull = n - n_null
        n_unique = int(s.nunique(dropna=True))
        unique_rate = (n_unique / n_nonnull) if n_nonnull else np.nan

        prof_rows.append({
            "col": c,
            "type": typ,
            "nulls": n_null,
            "pct_null": (n_null / n) if n else np.nan,
            "unique": n_unique,
            "unique_rate_nonnull": unique_rate,
            "is_probable_id": _is_probably_id(c) or (customer_id_col == c),
        })

    col_profile = pd.DataFrame(prof_rows).sort_values(["pct_null", "unique_rate_nonnull"], ascending=[False, False])
    out["column_profile"] = col_profile

    # Duplicates
    dup_rows = int(df.duplicated().sum())
    out["duplicate_rows"] = pd.DataFrame([{
        "duplicate_rows": dup_rows,
        "pct_duplicate_rows": (dup_rows / len(df)) if len(df) else np.nan
    }])

    # Customer ID duplication / uniqueness (if provided or inferred)
    cid = customer_id_col
    if cid is None:
        # try to infer a likely id col
        candidates = col_profile.sort_values(["is_probable_id", "unique_rate_nonnull"], ascending=[False, False])
        cid = candidates.iloc[0]["col"] if len(candidates) else None

    if cid and cid in df.columns:
        s = df[cid]
        out["customer_id_quality"] = pd.DataFrame([{
            "customer_id_col": cid,
            "nulls": int(s.isna().sum()),
            "unique_nonnull": int(s.nunique(dropna=True)),
            "rows": len(s),
            "pct_rows_unique_nonnull": (s.nunique(dropna=True) / (len(s) - s.isna().sum())) if (len(s) - s.isna().sum()) else np.nan,
            "duplicate_ids_nonnull": int(s.duplicated().sum()),
        }])
    else:
        out["customer_id_quality"] = pd.DataFrame([{
            "customer_id_col": None,
            "note": "No customer_id_col provided/found."
        }])

    # Numeric stats
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if num_cols:
        desc = df[num_cols].describe(percentiles=[0.01, 0.05, 0.5, 0.95, 0.99]).T
        desc["missing"] = df[num_cols].isna().sum()
        desc["pct_missing"] = desc["missing"] / len(df)
        out["numeric_stats"] = desc.reset_index(names="col").sort_values("pct_missing", ascending=False)
    else:
        out["numeric_stats"] = pd.DataFrame(columns=["col"])

    # Datetime range
    dt_cols = df.select_dtypes(include=["datetime64[ns]", "datetime64[ns, UTC]"]).columns.tolist()
    if dt_cols:
        dt_rows = []
        for c in dt_cols:
            s = df[c]
            dt_rows.append({
                "col": c,
                "min": s.min(),
                "max": s.max(),
                "nulls": int(s.isna().sum()),
                "pct_null": float(s.isna().mean()),
            })
        out["datetime_ranges"] = pd.DataFrame(dt_rows).sort_values("pct_null", ascending=False)
    else:
        out["datetime_ranges"] = pd.DataFrame(columns=["col", "min", "max", "nulls", "pct_null"])

    # Categorical/text top values
    cat_cols = [c for c in df.columns if _series_type_label(df[c]) in ("text", "category", "bool")]
    top_values: Dict[str, pd.DataFrame] = {}
    for c in cat_cols[:200]:  # avoid blowing up on ultra-wide tables
        s = df[c]
        # skip ultra-high cardinality columns unless ID-like
        if (s.nunique(dropna=True) > 5000) and (not _is_probably_id(c)):
            continue
        top_values[c] = _safe_value_counts(s, top_n=top_n)

    out["top_values"] = top_values

    # Quick correlation (numeric only, limited columns)
    if len(num_cols) >= 2:
        limited = num_cols[:50]  # cap to avoid huge matrix
        corr = df[limited].corr(numeric_only=True)
        out["corr_numeric"] = corr
    else:
        out["corr_numeric"] = pd.DataFrame()

    return out


# ---------------------------
# Orchestrator: df or SQL
# ---------------------------

def run_customer360_eda(
    data: Union[pd.DataFrame, SQLSource],
    *,
    sample_rows: int = 200_000,
    seed: int = 42,
    customer_id_col: Optional[str] = None,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    Main entry point.
    - If data is SQLSource: pulls a sample + runs pushdown row count & missingness if feasible.
    - If data is DataFrame: runs profiling directly (optionally sample if huge).
    """
    results: Dict[str, Any] = {}

    if isinstance(data, SQLSource):
        # SQL pushdowns
        try:
            results["row_count"] = pd.DataFrame([{"rows_total": data.row_count()}])
        except Exception as e:
            results["row_count"] = pd.DataFrame([{"rows_total": None, "error": str(e)}])

        try:
            results["missingness_sql"] = data.missingness()
        except Exception as e:
            results["missingness_sql"] = pd.DataFrame([{"error": str(e)}])

        # Sample to pandas for richer profiling
        df_sample = data.sample_df(n=sample_rows, seed=seed)
        results["sample_table_summary"] = pd.DataFrame([{
            "sample_rows": len(df_sample),
            "sample_cols": df_sample.shape[1],
            "sample_memory_mb_est": round(_approx_mem_mb(df_sample), 2)
        }])
        results.update({f"sample_{k}": v for k, v in profile_dataframe(
            df_sample, customer_id_col=customer_id_col, top_n=top_n
        ).items()})

        return results

    # Pandas DataFrame path
    df = data
    if len(df) > sample_rows:
        df = df.sample(n=sample_rows, random_state=seed)
        results["note"] = f"Input df had >{sample_rows:,} rows; profiled a sample of {len(df):,}."

    results.update(profile_dataframe(df, customer_id_col=customer_id_col, top_n=top_n))
    return results


# ---------------------------
# Example usage
# ---------------------------

if __name__ == "__main__":
    # ---- Option A: Already have a DataFrame df ----
    # results = run_customer360_eda(df, customer_id_col="customer_id")

    # ---- Option B: SQL table / query ----
    # pip install sqlalchemy psycopg2-binary (for Postgres), or pyodbc (for SQL Server), etc.
    from sqlalchemy import create_engine

    # Example (Postgres):
    # engine = create_engine("postgresql+psycopg2://user:password@host:5432/dbname")
    # src = SQLSource(engine=engine, table="customer360", schema="public")
    # results = run_customer360_eda(src, sample_rows=200_000, customer_id_col="customer_id")

    # Print a few key outputs if you run it
    # print(results["table_summary"])
    # print(results["column_profile"].head(25))
    pass


1Ô∏è‚É£ Use your existing pyodbc connection (unchanged login)
import pyodbc

conn = pyodbc.connect(
    f"DSN=DatalakeImpala;Schema=analyticsdb;UID={username};PWD={password}",
    autocommit=True
)

cursor = conn.cursor()


We‚Äôll reuse conn directly for pandas reads.

2Ô∏è‚É£ Replace SQLAlchemy with a pyodbc-based SQLSource

Here is a Impala-safe SQLSource that works with pyodbc + pandas.

import pandas as pd
import numpy as np
from dataclasses import dataclass
from typing import Optional


@dataclass
class ImpalaSQLSource:
    """
    SQL source backed by a pyodbc Impala connection
    """
    conn: pyodbc.Connection
    table: Optional[str] = None
    query: Optional[str] = None

    def base_sql(self) -> str:
        if self.query:
            return f"({self.query}) t"
        if self.table:
            return self.table
        raise ValueError("Provide either table or query")

    def sample_df(self, n: int = 100_000) -> pd.DataFrame:
        """
        Impala-friendly sampling:
        - LIMIT only (ORDER BY RAND() is expensive in Impala)
        """
        sql = f"""
        SELECT *
        FROM {self.base_sql()}
        LIMIT {n}
        """
        return pd.read_sql(sql, self.conn)

    def row_count(self) -> int:
        sql = f"""
        SELECT COUNT(*) AS n
        FROM {self.base_sql()}
        """
        return int(pd.read_sql(sql, self.conn).iloc[0, 0])

    def missingness(self) -> pd.DataFrame:
        """
        Column-wise null counts pushed to Impala
        """
        # Get columns without scanning data
        cols_sql = f"""
        SELECT *
        FROM {self.base_sql()}
        LIMIT 0
        """
        cols = pd.read_sql(cols_sql, self.conn).columns.tolist()

        null_exprs = [
            f"SUM(CASE WHEN {c} IS NULL THEN 1 ELSE 0 END) AS {c}"
            for c in cols
        ]

        sql = f"""
        SELECT {", ".join(null_exprs)}
        FROM {self.base_sql()}
        """

        nulls = pd.read_sql(sql, self.conn).iloc[0].to_frame("nulls")
        nulls["col"] = nulls.index
        nulls = nulls.reset_index(drop=True)[["col", "nulls"]]

        total_rows = self.row_count()
        nulls["pct_null"] = nulls["nulls"] / total_rows

        return nulls.sort_values("pct_null", ascending=False)

3Ô∏è‚É£ Updated example usage (Customer 360 table)
üëâ Table-based usage
src = ImpalaSQLSource(
    conn=conn,
    table="analyticsdb.customer_360"
)

results = run_customer360_eda(
    data=src,
    sample_rows=200_000,
    customer_id_col="customer_id"
)

üëâ Query-based usage (recommended for filtering early)
src = ImpalaSQLSource(
    conn=conn,
    query="""
        SELECT *
        FROM analyticsdb.customer_360
        WHERE is_active = 1
    """
)

results = run_customer360_eda(
    data=src,
    sample_rows=150_000,
    customer_id_col="customer_id"
)

